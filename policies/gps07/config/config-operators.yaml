---
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-console
  annotations:
    openshift.io/node-selector: node-role.kubernetes.io/master=
---
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-kube-storage-version-migrator
  annotations:
    openshift.io/node-selector: node-role.kubernetes.io/master=
---
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-network-console
  annotations:
    openshift.io/node-selector: node-role.kubernetes.io/infra=
---
apiVersion: self-node-remediation.medik8s.io/v1alpha1
kind: SelfNodeRemediationConfig
metadata:
  name: self-node-remediation-config
  namespace: openshift-workload-availability
spec:
  customDsTolerations:
  - effect: NoSchedule
    operator: Exists
    key: node-role.kubernetes.io/infra
  peerApiServerTimeout: 30s
  peerDialTimeout: 30s
  peerRequestTimeout: 30s
---
apiVersion: remediation.medik8s.io/v1alpha1
kind: NodeHealthCheck
metadata:
  name: nodehealthcheck-master
spec:
  minHealthy: 90%
  remediationTemplate:
    apiVersion: self-node-remediation.medik8s.io/v1alpha1
    name: self-node-remediation-automatic-strategy-template
    namespace: openshift-workload-availability
    kind: SelfNodeRemediationTemplate
  selector:
    matchExpressions:
    - key: node-role.kubernetes.io/master
      operator: Exists
  unhealthyConditions:
  - type: Ready
    status: "False"
    duration: 300s
  - type: Ready
    status: Unknown
    duration: 300s
---
apiVersion: remediation.medik8s.io/v1alpha1
kind: NodeHealthCheck
metadata:
  name: nodehealthcheck-worker
spec:
  minHealthy: 51%
  remediationTemplate:
    apiVersion: self-node-remediation.medik8s.io/v1alpha1
    name: self-node-remediation-automatic-strategy-template
    namespace: openshift-workload-availability
    kind: SelfNodeRemediationTemplate
  selector:
    matchExpressions:
    - key: node-role.kubernetes.io/worker
      operator: Exists
    - key: node-role.kubernetes.io/master
      operator: DoesNotExist
  unhealthyConditions:
  - type: Ready
    status: "False"
    duration: 300s
  - type: Ready
    status: Unknown
    duration: 300s
---
apiVersion: remediation.medik8s.io/v1alpha1
kind: NodeHealthCheck
metadata:
  name: nodehealthcheck-infra
spec:
  minHealthy: 51%
  remediationTemplate:
    apiVersion: self-node-remediation.medik8s.io/v1alpha1
    name: self-node-remediation-automatic-strategy-template
    namespace: openshift-workload-availability
    kind: SelfNodeRemediationTemplate
  selector:
    matchExpressions:
    - key: node-role.kubernetes.io/infra
      operator: Exists
    - key: node-role.kubernetes.io/worker
      operator: DoesNotExist
  unhealthyConditions:
  - type: Ready
    status: "False"
    duration: 300s
  - type: Ready
    status: Unknown
    duration: 300s
---
apiVersion: remediation.medik8s.io/v1alpha1
kind: NodeHealthCheck
metadata:
  name: nodehealthcheck-acm
spec:
  minHealthy: 51%
  remediationTemplate:
    apiVersion: self-node-remediation.medik8s.io/v1alpha1
    name: self-node-remediation-automatic-strategy-template
    namespace: openshift-workload-availability
    kind: SelfNodeRemediationTemplate
  selector:
    matchExpressions:
    - key: node-role.kubernetes.io/acm
      operator: Exists
    - key: node-role.kubernetes.io/worker
      operator: DoesNotExist
    - key: node-role.kubernetes.io/infra
      operator: DoesNotExist
  unhealthyConditions:
  - type: Ready
    status: "False"
    duration: 300s
  - type: Ready
    status: Unknown
    duration: 300s
---
kind: ServiceAccount
apiVersion: v1
metadata:
  name: eventrouter
  namespace: openshift-logging
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: event-reader
rules:
- apiGroups: [""]
  resources: ["events"]
  verbs: ["get", "watch", "list"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: event-reader-binding
subjects:
- kind: ServiceAccount
  name: eventrouter
  namespace: openshift-logging
roleRef:
  kind: ClusterRole
  name: event-reader
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: eventrouter
  namespace: openshift-logging
data:
  config.json: |-
    {
      "sink": "stdout"
    }
---
kind: Deployment
apiVersion: apps/v1
metadata:
  name: eventrouter
  namespace: openshift-logging
  labels:
    component: "eventrouter"
    logging-infra: "eventrouter"
    provider: "openshift"
spec:
  selector:
    matchLabels:
      component: "eventrouter"
      logging-infra: "eventrouter"
      provider: "openshift"
  replicas: 1
  template:
    metadata:
      labels:
        component: "eventrouter"
        logging-infra: "eventrouter"
        provider: "openshift"
      name: eventrouter
    spec:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      serviceAccount: eventrouter
      containers:
      - name: kube-eventrouter
        image: registry.redhat.io/openshift-logging/eventrouter-rhel9:v0.4
        imagePullPolicy: IfNotPresent
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
        volumeMounts:
        - name: config-volume
          mountPath: /etc/eventrouter
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      volumes:
      - name: config-volume
        configMap:
          name: eventrouter
---
kind: ServiceAccount
apiVersion: v1
metadata:
  name: log-collector
  namespace: openshift-logging
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: collect-application-logs
subjects:
- kind: ServiceAccount
  name: log-collector
  namespace: openshift-logging
roleRef:
  kind: ClusterRole
  name: collect-application-logs
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: collect-audit-logs
subjects:
- kind: ServiceAccount
  name: log-collector
  namespace: openshift-logging
roleRef:
  kind: ClusterRole
  name: collect-audit-logs
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: collect-infrastructure-logs
subjects:
- kind: ServiceAccount
  name: log-collector
  namespace: openshift-logging
roleRef:
  kind: ClusterRole
  name: collect-infrastructure-logs
---
apiVersion: policy.open-cluster-management.io/v1
kind: ConfigurationPolicy
metadata:
  name: config-operators-logforwarder
spec:
  remediationAction: enforce
  severity: high
  object-templates-raw: |
    {{hub- if (lookup "v1" "ConfigMap" "policies" "config-operators").data.logBrokers hub}}
    {{hub- if (lookup "v1" "ConfigMap" "policies" "config-operators").data.logTopic hub}}
    - complianceType: musthave
      recreateOption: IfRequired
      objectDefinition:
        apiVersion: observability.openshift.io/v1
        kind: ClusterLogForwarder
        metadata:
          name: log-forwarder
          namespace: openshift-logging
        spec:
          managementState: Managed
          collector:
            resources:
              limits:
                cpu: "1"
                memory: 2Gi
              requests:
                cpu: "1"
                memory: 2Gi
          serviceAccount:
            name: log-collector
          outputs:
          - name: kafka-receiver
            type: kafka
            kafka:
              brokers: {{hub (lookup "v1" "ConfigMap" "policies" "config-operators").data.logBrokers hub}}
              topic: {{hub (lookup "v1" "ConfigMap" "policies" "config-operators").data.logTopic hub}}
              tuning:
                deliveryMode: AtLeastOnce
                maxWrite: 10M
                compression: none
          pipelines:
          - name: audit-log
            inputRefs:
            - audit
            outputRefs:
            - kafka-receiver
            filterRefs:
            - add-cluster-name
          - name: infra-log
            inputRefs:
            - infrastructure
            outputRefs:
            - kafka-receiver
            filterRefs:
            - add-cluster-name
          filters:
          - name: add-cluster-name
            type: openshiftLabels
            openshiftLabels:
              cluster-name: {{hub .ManagedClusterName hub}}
    {{hub- end hub}}
    {{hub- end hub}}
---
apiVersion: policy.open-cluster-management.io/v1
kind: ConfigurationPolicy
metadata:
  name: config-operators-extra
spec:
  remediationAction: enforce
  severity: high
  object-templates-raw: |
    {{hub if ne "none" (index .ManagedClusterLabels "policies.extra") hub}}
    - complianceType: musthave
      objectDefinition:
        apiVersion: argoproj.io/v1beta1
        kind: ArgoCD
        metadata:
          name: openshift-gitops
          namespace: openshift-gitops
        spec:
          applicationSet:
            resources:
              limits:
                cpu: "2"
                memory: 1Gi
              requests:
                cpu: 250m
                memory: 512Mi
            webhookServer:
              ingress:
                enabled: false
              route:
                enabled: false
          controller:
            processors: {}
            resources:
              limits:
                cpu: "4"
                memory: 4Gi
              requests:
                cpu: 250m
                memory: 1Gi
          extraConfig:
            accounts.admin: apiKey
          grafana:
            enabled: false
            ingress:
              enabled: false
            resources:
              limits:
                cpu: 500m
                memory: 256Mi
              requests:
                cpu: 250m
                memory: 128Mi
            route:
              enabled: false
          ha:
            enabled: {{hub if eq "dev" (index .ManagedClusterLabels "policies.extra") hub}}false{{hub else hub}}true{{hub end hub}}
            resources:
              limits:
                cpu: 500m
                memory: 256Mi
              requests:
                cpu: 250m
                memory: 128Mi
          initialSSHKnownHosts: {}
          monitoring:
            enabled: false
          nodePlacement:
            nodeSelector:
              node-role.kubernetes.io/infra: ""
            tolerations:
            - effect: NoSchedule
              key: node-role.kubernetes.io/infra
              operator: Exists
          notifications:
            enabled: true
          prometheus:
            enabled: false
            ingress:
              enabled: false
            route:
              enabled: false
          rbac:
            defaultPolicy: ""
            policy: |
              g, system:cluster-admins, role:admin
              g, cluster-admin, role:admin
            scopes: '[groups]'
          redis:
            resources:
              limits:
                cpu: 500m
                memory: 256Mi
              requests:
                cpu: 250m
                memory: 128Mi
          repo:
            replicas: 2
            resources:
              limits:
                cpu: "1"
                memory: 1Gi
              requests:
                cpu: 250m
                memory: 256Mi
          resourceExclusions: |
            - apiGroups:
              - tekton.dev
              clusters:
              - '*'
              kinds:
              - TaskRun
              - PipelineRun
          server:
            autoscale:
              enabled: false
            grpc:
              ingress:
                enabled: false
            ingress:
              enabled: false
            replicas: 2
            resources:
              limits:
                cpu: 500m
                memory: 256Mi
              requests:
                cpu: 125m
                memory: 128Mi
            route:
              enabled: true
            service:
              type: ""
    {{hub end hub}}
---
apiVersion: policy.open-cluster-management.io/v1
kind: ConfigurationPolicy
metadata:
  name: config-operators-monitoring
spec:
  remediationAction: enforce
  severity: high
  object-templates-raw: |
    - complianceType: musthave
      objectDefinition:
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: cluster-monitoring-config
          namespace: openshift-monitoring
        data:
          config.yaml: |
            alertmanagerMain:
              nodeSelector:
                node-role.kubernetes.io/infra: ""
              tolerations:
              - effect: NoSchedule
                key: node-role.kubernetes.io/infra
                operator: Exists
    {{- if eq "Bound" (lookup "v1" "PersistentVolumeClaim" "openshift-monitoring" "alertmanager-main-db-alertmanager-main-0").status.phase }}
              volumeClaimTemplate:
                metadata:
                  name: alertmanager-main-db
                spec:
                  resources:
                    requests:
                      storage: 10Gi
    {{- end }}
            enableUserWorkload: false
            kubeStateMetrics:
              nodeSelector:
                node-role.kubernetes.io/infra: ""
              tolerations:
              - effect: NoSchedule
                key: node-role.kubernetes.io/infra
                operator: Exists
            metricsServer:
              nodeSelector:
                node-role.kubernetes.io/infra: ""
              tolerations:
              - effect: NoSchedule
                key: node-role.kubernetes.io/infra
                operator: Exists
            monitoringPlugin:
              nodeSelector:
                node-role.kubernetes.io/infra: ""
              tolerations:
              - effect: NoSchedule
                key: node-role.kubernetes.io/infra
                operator: Exists
            openshiftStateMetrics:
              nodeSelector:
                node-role.kubernetes.io/infra: ""
              tolerations:
              - effect: NoSchedule
                key: node-role.kubernetes.io/infra
                operator: Exists
            prometheusK8s:
              externalLabels:
                managed_cluster: {{hub .ManagedClusterName hub}}
              nodeSelector:
                node-role.kubernetes.io/infra: ""
    {{- if eq "Service" (lookup "v1" "Service" "clushkube-monitoring" "kube-prometheus-stack-prometheus").kind }}
              remoteWrite:
                - url: http://kube-prometheus-stack-prometheus.clushkube-monitoring.svc.cluster.local:9090/api/v1/write
    {{- end }}
              retentionSize: 60GiB
              tolerations:
              - effect: NoSchedule
                key: node-role.kubernetes.io/infra
                operator: Exists
    {{- if eq "Bound" (lookup "v1" "PersistentVolumeClaim" "openshift-monitoring" "prometheus-k8s-db-prometheus-k8s-0").status.phase }}
              volumeClaimTemplate:
                metadata:
                  name: prometheus-k8s-db
                spec:
                  resources:
                    requests:
                      storage: 90Gi
    {{- end }}
            prometheusOperator:
              nodeSelector:
                node-role.kubernetes.io/infra: ""
              tolerations:
              - effect: NoSchedule
                key: node-role.kubernetes.io/infra
                operator: Exists
            telemeterClient:
              nodeSelector:
                node-role.kubernetes.io/infra: ""
              tolerations: null
            thanosQuerier:
              nodeSelector:
                node-role.kubernetes.io/infra: ""
              tolerations:
              - effect: NoSchedule
                key: node-role.kubernetes.io/infra
                operator: Exists
---
apiVersion: policy.open-cluster-management.io/v1
kind: ConfigurationPolicy
metadata:
  name: config-operators-ingresscontroller
spec:
  remediationAction: enforce
  severity: high
  object-templates-raw: |
    {{- $rc := ( (lookup "machineconfiguration.openshift.io/v1" "MachineConfigPool" "" "infra").status.machineCount | toInt ) }}
    {{- if lt 1 $rc }}
    - complianceType: musthave
      objectDefinition:
        apiVersion: operator.openshift.io/v1
        kind: IngressController
        metadata:
          name: default
          namespace: openshift-ingress-operator
        spec:
          replicas: {{ $rc }}
    {{- end }}
---
