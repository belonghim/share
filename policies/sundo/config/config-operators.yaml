---
apiVersion: policy.open-cluster-management.io/v1
kind: ConfigurationPolicy
metadata:
  name: config-operators-clean-og
spec:
  remediationAction: enforce
  severity: high
  object-templates-raw: |
    {{- range $og := (lookup "operators.coreos.com/v1" "OperatorGroup" "" "" "policies.install").items }}
    {{- range $cg := (lookup "operators.coreos.com/v1" "OperatorGroup" $og.metadata.namespace "" "!policies.install").items }}
    - complianceType: mustnothave
      objectDefinition:
        apiVersion: operators.coreos.com/v1
        kind: OperatorGroup
        metadata:
          name: {{ $cg.metadata.name }}
          namespace: {{ $cg.metadata.namespace }}
    {{- end }}
    {{- end }}
---
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-kube-storage-version-migrator
  annotations:
    openshift.io/node-selector: node-role.kubernetes.io/master=
---
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-network-console
  annotations:
    openshift.io/node-selector: node-role.kubernetes.io/infra=
---
apiVersion: policy.open-cluster-management.io/v1
kind: ConfigurationPolicy
metadata:
  name: config-operators-snrc
spec:
  remediationAction: enforce
  severity: high
  object-templates-raw: |
    - complianceType: musthave
      objectDefinition:
        apiVersion: self-node-remediation.medik8s.io/v1alpha1
        kind: SelfNodeRemediationConfig
        metadata:
          name: self-node-remediation-config
          namespace: openshift-workload-availability
        spec:
    {{- range $cd := (lookup "config.openshift.io/v1" "ClusterVersion" "" "version").status.conditions }}
    {{- if eq "Progressing" $cd.type }}
    {{- if eq "True" $cd.status }}
          apiCheckInterval: 1h
          apiServerTimeout: 1h
    {{- else }}
          apiCheckInterval: 1m
          apiServerTimeout: 1m
    {{- end }}
    {{- end }}
    {{- end }}
    {{- range $ed := (lookup "operator.openshift.io/v1" "Etcd" "" "cluster").status.conditions }}
    {{- if eq "EtcdEndpointsDegraded" $ed.type }}
    {{- if eq "True" $ed.status }}
          maxApiErrorThreshold: 864000
    {{- end }}
    {{- end }}
    {{- end }}
          customDsTolerations:
          - effect: NoSchedule
            operator: Exists
            key: node-role.kubernetes.io/infra
---
apiVersion: remediation.medik8s.io/v1alpha1
kind: NodeHealthCheck
metadata:
  name: nodehealthcheck-master
spec:
  minHealthy: 51%
  remediationTemplate:
    apiVersion: self-node-remediation.medik8s.io/v1alpha1
    name: self-node-remediation-automatic-strategy-template
    namespace: openshift-workload-availability
    kind: SelfNodeRemediationTemplate
  selector:
    matchExpressions:
    - key: node-role.kubernetes.io/master
      operator: Exists
  unhealthyConditions:
  - type: Ready
    status: "False"
    duration: 300s
  - type: Ready
    status: Unknown
    duration: 300s
---
apiVersion: remediation.medik8s.io/v1alpha1
kind: NodeHealthCheck
metadata:
  name: nodehealthcheck-worker
spec:
  minHealthy: 51%
  remediationTemplate:
    apiVersion: self-node-remediation.medik8s.io/v1alpha1
    name: self-node-remediation-automatic-strategy-template
    namespace: openshift-workload-availability
    kind: SelfNodeRemediationTemplate
  selector:
    matchExpressions:
    - key: node-role.kubernetes.io/worker
      operator: Exists
    - key: node-role.kubernetes.io/master
      operator: DoesNotExist
  unhealthyConditions:
  - type: Ready
    status: "False"
    duration: 300s
  - type: Ready
    status: Unknown
    duration: 300s
---
apiVersion: remediation.medik8s.io/v1alpha1
kind: NodeHealthCheck
metadata:
  name: nodehealthcheck-infra
spec:
  minHealthy: 51%
  remediationTemplate:
    apiVersion: self-node-remediation.medik8s.io/v1alpha1
    name: self-node-remediation-automatic-strategy-template
    namespace: openshift-workload-availability
    kind: SelfNodeRemediationTemplate
  selector:
    matchExpressions:
    - key: node-role.kubernetes.io/infra
      operator: Exists
    - key: node-role.kubernetes.io/worker
      operator: DoesNotExist
  unhealthyConditions:
  - type: Ready
    status: "False"
    duration: 300s
  - type: Ready
    status: Unknown
    duration: 300s
---
apiVersion: remediation.medik8s.io/v1alpha1
kind: NodeHealthCheck
metadata:
  name: nodehealthcheck-acm
spec:
  minHealthy: 51%
  remediationTemplate:
    apiVersion: self-node-remediation.medik8s.io/v1alpha1
    name: self-node-remediation-automatic-strategy-template
    namespace: openshift-workload-availability
    kind: SelfNodeRemediationTemplate
  selector:
    matchExpressions:
    - key: node-role.kubernetes.io/acm
      operator: Exists
    - key: node-role.kubernetes.io/worker
      operator: DoesNotExist
    - key: node-role.kubernetes.io/infra
      operator: DoesNotExist
  unhealthyConditions:
  - type: Ready
    status: "False"
    duration: 300s
  - type: Ready
    status: Unknown
    duration: 300s
---
kind: Namespace
apiVersion: v1
metadata:
  name: ocp-logging
  annotations:
    openshift.io/node-selector: node-role.kubernetes.io/infra=
    scheduler.alpha.kubernetes.io/defaultTolerations: '[{"operator":"Exists","effect":"NoSchedule","key":"node-role.kubernetes.io/infra"}]'
---
kind: ServiceAccount
apiVersion: v1
metadata:
  name: eventrouter
  namespace: ocp-logging
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: event-reader
rules:
- apiGroups: [""]
  resources: ["events"]
  verbs: ["get", "watch", "list"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: event-reader-binding
subjects:
- kind: ServiceAccount
  name: eventrouter
  namespace: ocp-logging
roleRef:
  kind: ClusterRole
  name: event-reader
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: eventrouter
  namespace: ocp-logging
data:
  config.json: |-
    {
      "sink": "stdout"
    }
---
kind: Deployment
apiVersion: apps/v1
metadata:
  name: eventrouter
  namespace: ocp-logging
  labels:
    component: "eventrouter"
    logging-infra: "eventrouter"
    provider: "openshift"
spec:
  selector:
    matchLabels:
      component: "eventrouter"
      logging-infra: "eventrouter"
      provider: "openshift"
  replicas: 1
  template:
    metadata:
      labels:
        component: "eventrouter"
        logging-infra: "eventrouter"
        provider: "openshift"
      name: eventrouter
    spec:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      serviceAccount: eventrouter
      containers:
      - name: kube-eventrouter
        image: registry.redhat.io/openshift-logging/eventrouter-rhel9:v0.4
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
        volumeMounts:
        - name: config-volume
          mountPath: /etc/eventrouter
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      volumes:
      - name: config-volume
        configMap:
          name: eventrouter
---
kind: ServiceAccount
apiVersion: v1
metadata:
  name: log-collector
  namespace: openshift-logging
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: collect-application-logs
subjects:
- kind: ServiceAccount
  name: log-collector
  namespace: openshift-logging
roleRef:
  kind: ClusterRole
  name: collect-application-logs
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: collect-audit-logs
subjects:
- kind: ServiceAccount
  name: log-collector
  namespace: openshift-logging
roleRef:
  kind: ClusterRole
  name: collect-audit-logs
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: collect-infrastructure-logs
subjects:
- kind: ServiceAccount
  name: log-collector
  namespace: openshift-logging
roleRef:
  kind: ClusterRole
  name: collect-infrastructure-logs
---
apiVersion: policy.open-cluster-management.io/v1
kind: ConfigurationPolicy
metadata:
  name: config-operators-logforwarder
spec:
  remediationAction: enforce
  severity: high
  object-templates-raw: |
    {{hub- if and (lookup "v1" "ConfigMap" "policies" "config-operators").data.eventBrokers (ne "" (index .ManagedClusterLabels "policies.event-topic")) hub}}
    - complianceType: musthave
      recreateOption: IfRequired
      objectDefinition:
        apiVersion: observability.openshift.io/v1
        kind: ClusterLogForwarder
        metadata:
          name: log-forwarder
          namespace: openshift-logging
        spec:
          collector:
            resources:
              limits:
                cpu: "1"
                memory: 2Gi
              requests:
                cpu: "1"
                memory: 2Gi
            tolerations:
            - effect: NoSchedule
              key: node-role.kubernetes.io/infra
              operator: Exists
          serviceAccount:
            name: log-collector
          inputs:
          - name: eventrouter
            application:
              includes:
              - namespace: ocp-logging
              selector:
                matchLabels:
                  component: eventrouter
            type: application
          outputs:
    {{hub- if and (lookup "v1" "ConfigMap" "policies" "config-operators").data.infraBrokers (ne "" (index .ManagedClusterLabels "policies.infra-topic")) hub}}
          - name: infra-receiver
            kafka:
              brokers: {{hub (lookup "v1" "ConfigMap" "policies" "config-operators").data.infraBrokers hub}}
              topic: {{hub index .ManagedClusterLabels "policies.infra-topic" hub}}
              tuning:
                compression: none
                deliveryMode: AtLeastOnce
                maxWrite: 10M
            type: kafka
    {{hub- end hub}}
          - name: event-receiver
            kafka:
              brokers: {{hub (lookup "v1" "ConfigMap" "policies" "config-operators").data.eventBrokers hub}}
              topic: {{hub index .ManagedClusterLabels "policies.event-topic" hub}}
              tuning:
                compression: none
                deliveryMode: AtLeastOnce
                maxWrite: 10M
            type: kafka
    {{hub- $sl := index .ManagedClusterLabels "policies.syslog-url" hub}}
    {{hub- if ne "" $sl hub}}
          - name: rsyslog
            syslog:
              facility: user
              rfc: RFC5424
              severity: debug
              url: tcp://{{hub (splitn ".." 2 $sl)._0 hub}}:{{hub (splitn ".." 2 $sl)._1 hub}}
            type: syslog
    {{hub- end hub}}
          pipelines:
          - name: audit-log
            inputRefs:
            - audit
            outputRefs:
            - {{hub if ne "" $sl hub}}rsyslog{{hub else hub}}infra-receiver{{hub end hub}}
            filterRefs:
            - add-cluster-name
    {{hub- if and (lookup "v1" "ConfigMap" "policies" "config-operators").data.infraBrokers (ne "" (index .ManagedClusterLabels "policies.infra-topic")) hub}}
          - name: infra-log
            inputRefs:
            - infrastructure
            outputRefs:
            - infra-receiver
            filterRefs:
            - add-cluster-name
            - important
    {{hub- end hub}}
          - name: event-log
            inputRefs:
            - eventrouter
            outputRefs:
            - event-receiver
            filterRefs:
            - add-cluster-name
          filters:
          - name: add-cluster-name
            type: openshiftLabels
            openshiftLabels:
              cluster-name: {{ (lookup "config.openshift.io/v1" "Infrastructure" "" "cluster").status.infrastructureName }}
          - name: important
            type: drop
            drop:
            - test:
              - field: .message
                notMatches: "(?i)critical|error"
              - field: .level
                matches: "info|warning"
    {{hub- end hub}}
---
apiVersion: policy.open-cluster-management.io/v1
kind: ConfigurationPolicy
metadata:
  name: config-operators-extra
spec:
  remediationAction: enforce
  severity: high
  object-templates-raw: |
    {{hub if ne "none" (index .ManagedClusterLabels "policies.extra") hub}}
    - complianceType: musthave
      objectDefinition:
        apiVersion: argoproj.io/v1beta1
        kind: ArgoCD
        metadata:
          name: openshift-gitops
          namespace: openshift-gitops
        spec:
          applicationSet:
            resources:
              limits:
                cpu: "2"
                memory: 1Gi
              requests:
                cpu: 250m
                memory: 512Mi
            webhookServer:
              ingress:
                enabled: false
              route:
                enabled: false
          controller:
            processors: {}
            resources:
              limits:
                cpu: "4"
                memory: 4Gi
              requests:
                cpu: 250m
                memory: 1Gi
          extraConfig:
            accounts.admin: apiKey
          grafana:
            enabled: false
            ingress:
              enabled: false
            resources:
              limits:
                cpu: 500m
                memory: 256Mi
              requests:
                cpu: 250m
                memory: 128Mi
            route:
              enabled: false
          ha:
            enabled: {{hub if eq "dev" (index .ManagedClusterLabels "policies.extra") hub}}false{{hub else hub}}true{{hub end hub}}
            resources:
              limits:
                cpu: 500m
                memory: 256Mi
              requests:
                cpu: 250m
                memory: 128Mi
          initialSSHKnownHosts: {}
          monitoring:
            enabled: false
          nodePlacement:
            nodeSelector:
              node-role.kubernetes.io/worker: ""
            tolerations:
            - effect: NoSchedule
              key: node-role.kubernetes.io/infra
              operator: Exists
          notifications:
            enabled: true
          prometheus:
            enabled: false
            ingress:
              enabled: false
            route:
              enabled: false
          rbac:
            defaultPolicy: ""
            policy: |
              g, system:cluster-admins, role:admin
              g, cluster-admin, role:admin
            scopes: '[groups]'
          redis:
            resources:
              limits:
                cpu: 500m
                memory: 256Mi
              requests:
                cpu: 250m
                memory: 128Mi
          repo:
            replicas: 2
            resources:
              limits:
                cpu: "1"
                memory: 1Gi
              requests:
                cpu: 250m
                memory: 256Mi
          resourceExclusions: |
            - apiGroups:
              - tekton.dev
              clusters:
              - '*'
              kinds:
              - TaskRun
              - PipelineRun
          server:
            autoscale:
              enabled: false
            grpc:
              ingress:
                enabled: false
            ingress:
              enabled: false
            replicas: 2
            resources:
              limits:
                cpu: 500m
                memory: 256Mi
              requests:
                cpu: 125m
                memory: 128Mi
            route:
              enabled: true
            service:
              type: ""
    {{hub end hub}}
---
apiVersion: policy.open-cluster-management.io/v1
kind: ConfigurationPolicy
metadata:
  name: config-operators-monitoring
spec:
  remediationAction: enforce
  severity: high
  object-templates-raw: |
    - complianceType: musthave
      objectDefinition:
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: cluster-monitoring-config
          namespace: openshift-monitoring
        data:
          config.yaml: |
            alertmanagerMain:
              nodeSelector:
                node-role.kubernetes.io/infra: ""
              tolerations:
              - effect: NoSchedule
                key: node-role.kubernetes.io/infra
                operator: Exists
    {{- if eq "Bound" (lookup "v1" "PersistentVolumeClaim" "openshift-monitoring" "alertmanager-main-db-alertmanager-main-0").status.phase }}
              volumeClaimTemplate:
                metadata:
                  name: alertmanager-main-db
                spec:
                  resources:
                    requests:
                      storage: 10Gi
    {{- end }}
            enableUserWorkload: false
            kubeStateMetrics:
              nodeSelector:
                node-role.kubernetes.io/infra: ""
              tolerations:
              - effect: NoSchedule
                key: node-role.kubernetes.io/infra
                operator: Exists
            metricsServer:
              nodeSelector:
                node-role.kubernetes.io/infra: ""
              tolerations:
              - effect: NoSchedule
                key: node-role.kubernetes.io/infra
                operator: Exists
            monitoringPlugin:
              nodeSelector:
                node-role.kubernetes.io/infra: ""
              tolerations:
              - effect: NoSchedule
                key: node-role.kubernetes.io/infra
                operator: Exists
            openshiftStateMetrics:
              nodeSelector:
                node-role.kubernetes.io/infra: ""
              tolerations:
              - effect: NoSchedule
                key: node-role.kubernetes.io/infra
                operator: Exists
            prometheusK8s:
              externalLabels:
                managed_cluster: {{ (lookup "config.openshift.io/v1" "Infrastructure" "" "cluster").status.infrastructureName }}
              nodeSelector:
                node-role.kubernetes.io/infra: ""
    {{- $np := "{{hub index .ManagedClusterLabels "policies.ns-prefix" hub}}" }}
    {{- $mn := "clushkube-monitoring" }}
    {{- if ne "" $np }}{{ $mn = (cat $np "-monitoring-ns" | replace " " "") }}{{ end }}
              remoteWrite:
              - url: http://kube-prometheus-stack-prometheus.{{ $mn }}.svc.cluster.local:9090/api/v1/write
              retentionSize: 30GiB
              tolerations:
              - effect: NoSchedule
                key: node-role.kubernetes.io/infra
                operator: Exists
    {{- if eq "Bound" (lookup "v1" "PersistentVolumeClaim" "openshift-monitoring" "prometheus-k8s-db-prometheus-k8s-0").status.phase }}
              volumeClaimTemplate:
                metadata:
                  name: prometheus-k8s-db
                spec:
                  resources:
                    requests:
                      storage: 90Gi
    {{- end }}
            prometheusOperator:
              nodeSelector:
                node-role.kubernetes.io/infra: ""
              tolerations:
              - effect: NoSchedule
                key: node-role.kubernetes.io/infra
                operator: Exists
            telemeterClient:
              nodeSelector:
                node-role.kubernetes.io/infra: ""
              tolerations: null
            thanosQuerier:
              nodeSelector:
                node-role.kubernetes.io/infra: ""
              tolerations:
              - effect: NoSchedule
                key: node-role.kubernetes.io/infra
                operator: Exists
---
apiVersion: policy.open-cluster-management.io/v1
kind: ConfigurationPolicy
metadata:
  name: config-operators-ingresscontroller
spec:
  remediationAction: enforce
  severity: high
  object-templates-raw: |
    {{- $rc := ( (lookup "machineconfiguration.openshift.io/v1" "MachineConfigPool" "" "infra").status.machineCount | toInt ) }}
    {{- if lt 1 $rc }}
    - complianceType: musthave
      objectDefinition:
        apiVersion: operator.openshift.io/v1
        kind: IngressController
        metadata:
          name: default
          namespace: openshift-ingress-operator
        spec:
          replicas: {{ $rc }}
    {{- end }}
---
